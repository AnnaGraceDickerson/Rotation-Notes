{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37526e83-7cbc-48b4-9586-42b36937f2a7",
   "metadata": {},
   "source": [
    "# Creating a CNN with Pytorch\n",
    "[source](https://machinelearningmastery.com/develop-your-first-neural-network-with-pytorch-step-by-step/)\n",
    "1. Get a dataset and separate your input features into X Tensor and your output answer values into a y Tensor reshaped into 1 column\n",
    "2. Create your CNN class that inherits from nn.Module and defines the layers and forward pass\n",
    "3. Define your loss function and optimizer\n",
    "4. Choose epochs and batches and train your model (epochs outer loop, batches inner loop)\n",
    "   1. predict answer based on inputs `y_pred = model(x_batch)`\n",
    "   2. get the loss `loss = loss_fn(y_pred, y)`\n",
    "   3. get the zero gradient of the loss function `optimizer.zero_grad()`\n",
    "   4. do a backwards pass to compute the gradients `loss.backward()`\n",
    "   5. update the model parameters `optimizer.step()`\n",
    "   6. Print out how the model is doing during training `print(f'Finished epoch {epoch}, latest loss {loss}')`\n",
    "\n",
    "Save model:\n",
    "`torch.save(model.state_dict(), 'model.pth')`\n",
    "\n",
    "Load model (choose if you only want the weights; if you want to tune the model more don't say weights_only):\n",
    "\n",
    "`model = MyModelClassNameOrConstructor()`\n",
    "\n",
    "`model.load_state_dict(torch.load('model.pth', weights_only=True))`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a110565a-21ab-45b7-87bb-1f9593d90b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "4c2aba8e-7bec-42d3-b409-bdf1648f5bba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "56b9147a-db73-4149-b224-c08333bb916c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.Tensor(X)\n",
    "y = torch.Tensor(y).reshape(-1,1)\n",
    "# note: Adding -1 to the shape just lets numpy calculate the remaining value for you, \n",
    "# so that the product of the axes still matches the previous number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0519ff59-313c-4c8f-9776-aca05b82c066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([768, 8])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc53c42f-7cad-4514-bc64-abea0ec2a205",
   "metadata": {},
   "source": [
    "In this approach, a class needs to have all the layers defined in the constructor because you need to prepare all its components when it is created, but the input is not yet provided. Note that you also need to call the parent classâ€™s constructor (the line super().__init__()) to bootstrap your model. You also need to define a forward() function in the class to tell, if an input tensor x is provided, how you produce the output tensor in return."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562f0e28-ef58-4047-8b0b-5690253e57a2",
   "metadata": {},
   "source": [
    "nn.Module is a fundamental building block in PyTorch, used for defining neural network models. It provides a way to create custom neural network architectures by defining layers and their forward pass. \n",
    "\n",
    "To use nn.Module, you need to:\n",
    "1. create a new class that inherits from nn.Module\n",
    "2. define the\n",
    "- layers\n",
    "-  the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae773488-ca63-4734-806f-9355198fd644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5e1720ed-b2d1-46a7-8922-de8d83bc5a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cd469-956c-4d64-96e5-8dd4cc0ddaf8",
   "metadata": {},
   "source": [
    "The goal of training a model is to ensure it learns a good enough mapping of input data to output classification. It will not be perfect, and errors are inevitable. Usually, you will see the amount of error reducing when in the later epochs, but it will eventually level out. This is called model convergence.\n",
    "\n",
    "The simplest way to build a training loop is to use two nested for-loops, one for epochs and one for batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a9106d38-b920-49e5-aab6-2107d33c68a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished epoch 0, latest loss 0.34976398944854736\n",
      "Finished epoch 0, latest loss 0.34976398944854736\n",
      "Finished epoch 1, latest loss 0.3472548723220825\n",
      "Finished epoch 1, latest loss 0.3472548723220825\n",
      "Finished epoch 2, latest loss 0.3426494002342224\n",
      "Finished epoch 2, latest loss 0.3426494002342224\n",
      "Finished epoch 3, latest loss 0.3417043387889862\n",
      "Finished epoch 3, latest loss 0.3417043387889862\n",
      "Finished epoch 4, latest loss 0.34253567457199097\n",
      "Finished epoch 4, latest loss 0.34253567457199097\n",
      "Finished epoch 5, latest loss 0.34057021141052246\n",
      "Finished epoch 5, latest loss 0.34057021141052246\n",
      "Finished epoch 6, latest loss 0.3380061686038971\n",
      "Finished epoch 6, latest loss 0.3380061686038971\n",
      "Finished epoch 7, latest loss 0.3432692587375641\n",
      "Finished epoch 7, latest loss 0.3432692587375641\n",
      "Finished epoch 8, latest loss 0.3439106345176697\n",
      "Finished epoch 8, latest loss 0.3439106345176697\n",
      "Finished epoch 9, latest loss 0.33861076831817627\n",
      "Finished epoch 9, latest loss 0.33861076831817627\n",
      "Finished epoch 10, latest loss 0.3296877145767212\n",
      "Finished epoch 10, latest loss 0.3296877145767212\n",
      "Finished epoch 11, latest loss 0.33182215690612793\n",
      "Finished epoch 11, latest loss 0.33182215690612793\n",
      "Finished epoch 12, latest loss 0.33527493476867676\n",
      "Finished epoch 12, latest loss 0.33527493476867676\n",
      "Finished epoch 13, latest loss 0.3252885639667511\n",
      "Finished epoch 13, latest loss 0.3252885639667511\n",
      "Finished epoch 14, latest loss 0.32962608337402344\n",
      "Finished epoch 14, latest loss 0.32962608337402344\n",
      "Finished epoch 15, latest loss 0.3278499245643616\n",
      "Finished epoch 15, latest loss 0.3278499245643616\n",
      "Finished epoch 16, latest loss 0.32563942670822144\n",
      "Finished epoch 16, latest loss 0.32563942670822144\n",
      "Finished epoch 17, latest loss 0.3233031630516052\n",
      "Finished epoch 17, latest loss 0.3233031630516052\n",
      "Finished epoch 18, latest loss 0.3209172189235687\n",
      "Finished epoch 18, latest loss 0.3209172189235687\n",
      "Finished epoch 19, latest loss 0.3159911632537842\n",
      "Finished epoch 19, latest loss 0.3159911632537842\n",
      "Finished epoch 20, latest loss 0.3163573145866394\n",
      "Finished epoch 20, latest loss 0.3163573145866394\n",
      "Finished epoch 21, latest loss 0.314179927110672\n",
      "Finished epoch 21, latest loss 0.314179927110672\n",
      "Finished epoch 22, latest loss 0.3175503611564636\n",
      "Finished epoch 22, latest loss 0.3175503611564636\n",
      "Finished epoch 23, latest loss 0.3135027587413788\n",
      "Finished epoch 23, latest loss 0.3135027587413788\n",
      "Finished epoch 24, latest loss 0.31033214926719666\n",
      "Finished epoch 24, latest loss 0.31033214926719666\n",
      "Finished epoch 25, latest loss 0.3109332025051117\n",
      "Finished epoch 25, latest loss 0.3109332025051117\n",
      "Finished epoch 26, latest loss 0.30929097533226013\n",
      "Finished epoch 26, latest loss 0.30929097533226013\n",
      "Finished epoch 27, latest loss 0.31679752469062805\n",
      "Finished epoch 27, latest loss 0.31679752469062805\n",
      "Finished epoch 28, latest loss 0.30941593647003174\n",
      "Finished epoch 28, latest loss 0.30941593647003174\n",
      "Finished epoch 29, latest loss 0.3048292100429535\n",
      "Finished epoch 29, latest loss 0.3048292100429535\n",
      "Finished epoch 30, latest loss 0.30689722299575806\n",
      "Finished epoch 30, latest loss 0.30689722299575806\n",
      "Finished epoch 31, latest loss 0.30458495020866394\n",
      "Finished epoch 31, latest loss 0.30458495020866394\n",
      "Finished epoch 32, latest loss 0.30455392599105835\n",
      "Finished epoch 32, latest loss 0.30455392599105835\n",
      "Finished epoch 33, latest loss 0.3066531717777252\n",
      "Finished epoch 33, latest loss 0.3066531717777252\n",
      "Finished epoch 34, latest loss 0.30627891421318054\n",
      "Finished epoch 34, latest loss 0.30627891421318054\n",
      "Finished epoch 35, latest loss 0.30312028527259827\n",
      "Finished epoch 35, latest loss 0.30312028527259827\n",
      "Finished epoch 36, latest loss 0.29743823409080505\n",
      "Finished epoch 36, latest loss 0.29743823409080505\n",
      "Finished epoch 37, latest loss 0.29909005761146545\n",
      "Finished epoch 37, latest loss 0.29909005761146545\n",
      "Finished epoch 38, latest loss 0.2899521291255951\n",
      "Finished epoch 38, latest loss 0.2899521291255951\n",
      "Finished epoch 39, latest loss 0.2941504418849945\n",
      "Finished epoch 39, latest loss 0.2941504418849945\n",
      "Finished epoch 40, latest loss 0.2957512140274048\n",
      "Finished epoch 40, latest loss 0.2957512140274048\n",
      "Finished epoch 41, latest loss 0.2933904230594635\n",
      "Finished epoch 41, latest loss 0.2933904230594635\n",
      "Finished epoch 42, latest loss 0.28789958357810974\n",
      "Finished epoch 42, latest loss 0.28789958357810974\n",
      "Finished epoch 43, latest loss 0.29149526357650757\n",
      "Finished epoch 43, latest loss 0.29149526357650757\n",
      "Finished epoch 44, latest loss 0.29122260212898254\n",
      "Finished epoch 44, latest loss 0.29122260212898254\n",
      "Finished epoch 45, latest loss 0.28694453835487366\n",
      "Finished epoch 45, latest loss 0.28694453835487366\n",
      "Finished epoch 46, latest loss 0.28840625286102295\n",
      "Finished epoch 46, latest loss 0.28840625286102295\n",
      "Finished epoch 47, latest loss 0.28618544340133667\n",
      "Finished epoch 47, latest loss 0.28618544340133667\n",
      "Finished epoch 48, latest loss 0.29258862137794495\n",
      "Finished epoch 48, latest loss 0.29258862137794495\n",
      "Finished epoch 49, latest loss 0.28730669617652893\n",
      "Finished epoch 49, latest loss 0.28730669617652893\n",
      "Finished epoch 50, latest loss 0.29066258668899536\n",
      "Finished epoch 50, latest loss 0.29066258668899536\n",
      "Finished epoch 51, latest loss 0.2919519543647766\n",
      "Finished epoch 51, latest loss 0.2919519543647766\n",
      "Finished epoch 52, latest loss 0.28800973296165466\n",
      "Finished epoch 52, latest loss 0.28800973296165466\n",
      "Finished epoch 53, latest loss 0.2892879843711853\n",
      "Finished epoch 53, latest loss 0.2892879843711853\n",
      "Finished epoch 54, latest loss 0.29044920206069946\n",
      "Finished epoch 54, latest loss 0.29044920206069946\n",
      "Finished epoch 55, latest loss 0.2865196764469147\n",
      "Finished epoch 55, latest loss 0.2865196764469147\n",
      "Finished epoch 56, latest loss 0.2859650254249573\n",
      "Finished epoch 56, latest loss 0.2859650254249573\n",
      "Finished epoch 57, latest loss 0.3005949556827545\n",
      "Finished epoch 57, latest loss 0.3005949556827545\n",
      "Finished epoch 58, latest loss 0.27807965874671936\n",
      "Finished epoch 58, latest loss 0.27807965874671936\n",
      "Finished epoch 59, latest loss 0.2797889709472656\n",
      "Finished epoch 59, latest loss 0.2797889709472656\n",
      "Finished epoch 60, latest loss 0.27181291580200195\n",
      "Finished epoch 60, latest loss 0.27181291580200195\n",
      "Finished epoch 61, latest loss 0.299075186252594\n",
      "Finished epoch 61, latest loss 0.299075186252594\n",
      "Finished epoch 62, latest loss 0.27489712834358215\n",
      "Finished epoch 62, latest loss 0.27489712834358215\n",
      "Finished epoch 63, latest loss 0.2825109362602234\n",
      "Finished epoch 63, latest loss 0.2825109362602234\n",
      "Finished epoch 64, latest loss 0.28888970613479614\n",
      "Finished epoch 64, latest loss 0.28888970613479614\n",
      "Finished epoch 65, latest loss 0.2736918330192566\n",
      "Finished epoch 65, latest loss 0.2736918330192566\n",
      "Finished epoch 66, latest loss 0.2775556743144989\n",
      "Finished epoch 66, latest loss 0.2775556743144989\n",
      "Finished epoch 67, latest loss 0.29101118445396423\n",
      "Finished epoch 67, latest loss 0.29101118445396423\n",
      "Finished epoch 68, latest loss 0.27350854873657227\n",
      "Finished epoch 68, latest loss 0.27350854873657227\n",
      "Finished epoch 69, latest loss 0.2749073803424835\n",
      "Finished epoch 69, latest loss 0.2749073803424835\n",
      "Finished epoch 70, latest loss 0.2749452292919159\n",
      "Finished epoch 70, latest loss 0.2749452292919159\n",
      "Finished epoch 71, latest loss 0.27283066511154175\n",
      "Finished epoch 71, latest loss 0.27283066511154175\n",
      "Finished epoch 72, latest loss 0.27548840641975403\n",
      "Finished epoch 72, latest loss 0.27548840641975403\n",
      "Finished epoch 73, latest loss 0.2753392457962036\n",
      "Finished epoch 73, latest loss 0.2753392457962036\n",
      "Finished epoch 74, latest loss 0.2755570113658905\n",
      "Finished epoch 74, latest loss 0.2755570113658905\n",
      "Finished epoch 75, latest loss 0.27489960193634033\n",
      "Finished epoch 75, latest loss 0.27489960193634033\n",
      "Finished epoch 76, latest loss 0.2739923894405365\n",
      "Finished epoch 76, latest loss 0.2739923894405365\n",
      "Finished epoch 77, latest loss 0.2749057412147522\n",
      "Finished epoch 77, latest loss 0.2749057412147522\n",
      "Finished epoch 78, latest loss 0.2754656970500946\n",
      "Finished epoch 78, latest loss 0.2754656970500946\n",
      "Finished epoch 79, latest loss 0.27443066239356995\n",
      "Finished epoch 79, latest loss 0.27443066239356995\n",
      "Finished epoch 80, latest loss 0.27621644735336304\n",
      "Finished epoch 80, latest loss 0.27621644735336304\n",
      "Finished epoch 81, latest loss 0.26904812455177307\n",
      "Finished epoch 81, latest loss 0.26904812455177307\n",
      "Finished epoch 82, latest loss 0.2744067907333374\n",
      "Finished epoch 82, latest loss 0.2744067907333374\n",
      "Finished epoch 83, latest loss 0.27568575739860535\n",
      "Finished epoch 83, latest loss 0.27568575739860535\n",
      "Finished epoch 84, latest loss 0.26704660058021545\n",
      "Finished epoch 84, latest loss 0.26704660058021545\n",
      "Finished epoch 85, latest loss 0.2685438096523285\n",
      "Finished epoch 85, latest loss 0.2685438096523285\n",
      "Finished epoch 86, latest loss 0.2836625874042511\n",
      "Finished epoch 86, latest loss 0.2836625874042511\n",
      "Finished epoch 87, latest loss 0.273988276720047\n",
      "Finished epoch 87, latest loss 0.273988276720047\n",
      "Finished epoch 88, latest loss 0.2705768346786499\n",
      "Finished epoch 88, latest loss 0.2705768346786499\n",
      "Finished epoch 89, latest loss 0.2698853015899658\n",
      "Finished epoch 89, latest loss 0.2698853015899658\n",
      "Finished epoch 90, latest loss 0.2715933322906494\n",
      "Finished epoch 90, latest loss 0.2715933322906494\n",
      "Finished epoch 91, latest loss 0.2652742564678192\n",
      "Finished epoch 91, latest loss 0.2652742564678192\n",
      "Finished epoch 92, latest loss 0.2732909321784973\n",
      "Finished epoch 92, latest loss 0.2732909321784973\n",
      "Finished epoch 93, latest loss 0.2670712172985077\n",
      "Finished epoch 93, latest loss 0.2670712172985077\n",
      "Finished epoch 94, latest loss 0.28609105944633484\n",
      "Finished epoch 94, latest loss 0.28609105944633484\n",
      "Finished epoch 95, latest loss 0.2753817141056061\n",
      "Finished epoch 95, latest loss 0.2753817141056061\n",
      "Finished epoch 96, latest loss 0.2688252925872803\n",
      "Finished epoch 96, latest loss 0.2688252925872803\n",
      "Finished epoch 97, latest loss 0.26331913471221924\n",
      "Finished epoch 97, latest loss 0.26331913471221924\n",
      "Finished epoch 98, latest loss 0.26844754815101624\n",
      "Finished epoch 98, latest loss 0.26844754815101624\n",
      "Finished epoch 99, latest loss 0.26406723260879517\n",
      "Finished epoch 99, latest loss 0.26406723260879517\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f'Finished epoch {epoch}, latest loss {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd3e5fd-71a2-4b3c-bc3a-60086870fdcf",
   "metadata": {},
   "source": [
    "You have trained our neural network on the entire dataset, and you can evaluate the performance of the network on the same dataset. This will only give you an idea of how well you have modeled the dataset (e.g., train accuracy) but no idea of how well the algorithm might perform on new data. This was done for simplicity, but ideally, you could separate your data into train and test datasets for training and evaluation of your model.\n",
    "\n",
    "You can evaluate your model on your training dataset in the same way you invoked the model in training. This will generate predictions for each input, but then you still need to compute a score for the evaluation. This score can be the same as your loss function or something different. Because you are doing binary classification, you can use accuracy as your evaluation score by converting the output (a floating point in the range of 0 to 1) to an integer (0 or 1) and compare to the label we know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dd7aad1a-d44c-47e0-8950-13b3230dfbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.7734375\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_pred = model(X)\n",
    "\n",
    "# gets the output of the sigmoid function (probability that the answer will be yes), makes it predict either yes or no based on if it's closer to 1 or 0, checks\n",
    "# if each prediction was right, adds up the right guesses and averages them -> \"accuracy\"\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b338f-1b87-40b2-808e-909bd59a98ed",
   "metadata": {},
   "source": [
    "using a sigmoid activation function on the output layer so that the predictions will be a probability in the range between 0 and 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3727e66f-7a24-4edd-83f0-e1deee2e0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make probability predictions with the model\n",
    "probability_predictions = model(X)\n",
    "# round predictions\n",
    "rounded = predictions.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c5c7b21c-78fd-410a-90fb-df68245ed0ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities: tensor([[0.7903],\n",
      "        [0.1231],\n",
      "        [0.8872],\n",
      "        [0.1254],\n",
      "        [0.7991]], grad_fn=<SliceBackward0>) Predictions: tensor([[1.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [1.]], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(\"Probabilities:\", probability_predictions[0:5,:], \"Predictions:\", rounded[0:5,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ef67a4b8-07a2-4609-821b-f1590834e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions with the model\n",
    "predictions = (model(X) > 0.5).int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b35815ef-7def-442c-812e-a1f1da9a2a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class predictions: tensor([[1],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0],\n",
      "        [1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(\"Class predictions:\", predictions[0:5, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a676738c-77a8-4135-ab61-08e50c948c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They're equivalent.\n"
     ]
    }
   ],
   "source": [
    "if (rounded.int().detach().numpy() == predictions.detach().numpy()).all():\n",
    "    print(\"They're equivalent.\")\n",
    "else:\n",
    "    print(\"They are not equivalent.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744a4152-dbc2-466e-ae8c-daf04b1d3335",
   "metadata": {},
   "source": [
    "This code uses a different way of building the model but should functionally be the same as before. After the model is trained, predictions are made for all examples in the dataset, and the input rows and predicted class value for the first five examples are printed and compared to the expected class value. You can see that most rows are correctly predicted. In fact, you can expect about 77% of the rows to be correctly predicted based on your estimated performance of the model in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "dbfeeb95-5f73-48eb-a072-7fd641b9427c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PimaClassifier(\n",
      "  (hidden1): Linear(in_features=8, out_features=12, bias=True)\n",
      "  (act1): ReLU()\n",
      "  (hidden2): Linear(in_features=12, out_features=8, bias=True)\n",
      "  (act2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=1, bias=True)\n",
      "  (act_output): Sigmoid()\n",
      ")\n",
      "Accuracy 0.7734375\n",
      "8 inputs: [6.0, 148.0, 72.0, 35.0, 0.0, 33.599998474121094, 0.6269999742507935, 50.0] => 1 (expected/correct value 1)\n",
      "8 inputs: [1.0, 85.0, 66.0, 29.0, 0.0, 26.600000381469727, 0.35100001096725464, 31.0] => 0 (expected/correct value 0)\n",
      "8 inputs: [8.0, 183.0, 64.0, 0.0, 0.0, 23.299999237060547, 0.671999990940094, 32.0] => 1 (expected/correct value 1)\n",
      "8 inputs: [1.0, 89.0, 66.0, 23.0, 94.0, 28.100000381469727, 0.16699999570846558, 21.0] => 0 (expected/correct value 0)\n",
      "8 inputs: [0.0, 137.0, 40.0, 35.0, 168.0, 43.099998474121094, 2.2880001068115234, 33.0] => 1 (expected/correct value 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# load the dataset, split into input (X) and output (y) variables\n",
    "dataset = np.loadtxt('pima-indians-diabetes.csv', delimiter=',')\n",
    "X = dataset[:,0:8]\n",
    "y = dataset[:,8]\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.float32).reshape(-1, 1)\n",
    "\n",
    "# define the model\n",
    "class PimaClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(8, 12)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.hidden2 = nn.Linear(12, 8)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.output = nn.Linear(8, 1)\n",
    "        self.act_output = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act_output(self.output(x))\n",
    "        return x\n",
    "\n",
    "model = PimaClassifier()\n",
    "print(model)\n",
    "\n",
    "# train the model\n",
    "loss_fn   = nn.BCELoss()  # binary cross entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 10\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(0, len(X), batch_size):\n",
    "        Xbatch = X[i:i+batch_size]\n",
    "        y_pred = model(Xbatch)\n",
    "        ybatch = y[i:i+batch_size]\n",
    "        loss = loss_fn(y_pred, ybatch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# compute accuracy\n",
    "y_pred = model(X)\n",
    "accuracy = (y_pred.round() == y).float().mean()\n",
    "print(f\"Accuracy {accuracy}\")\n",
    "\n",
    "# make class predictions with the model\n",
    "predictions = (model(X) > 0.5).int()\n",
    "for i in range(5):\n",
    "    print('8 inputs: %s => %d (expected/correct value %d)' % (X[i].tolist(), predictions[i], y[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d47a3e-2b04-4a6e-ab77-33633b4766c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
