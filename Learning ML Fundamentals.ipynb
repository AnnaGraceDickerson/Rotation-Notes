{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff4c4158-ba08-4e3a-b898-bf6de4748a7c",
   "metadata": {},
   "source": [
    "# Time-series CNN forecasting\n",
    "[source](https://machinelearningmastery.com/how-to-develop-convolutional-neural-network-models-for-time-series-forecasting/)\n",
    "\n",
    "\n",
    "#### 7 Options:\n",
    "\n",
    "*Predict Binary Class*\n",
    "- **Univariate CNN Models**\n",
    "   1.  Raw data -> binary class\n",
    "   2.  Only high pass -> binary class\n",
    "- **Multivariate CNN Models**\n",
    "   - **Multiple Input Series**\n",
    "      3. Spike-sorted/single-unit -> binary class\n",
    "   - **Multiple Parallel Series**\n",
    "\n",
    "*Predict voltage instead of binary class, then feed to a seizure classification algorithm*\n",
    "- **Multi-Step CNN Models**\n",
    "   4. Raw data -> raw data \n",
    "   5. Only high pass -> only high pass\n",
    "- **Multivariate Multi-Step CNN Models**\n",
    "    - **Multiple Input Multi-Step Output**\n",
    "       6. Spike-sorted/single-unit -> high pass\n",
    "    - **Multiple Parallel Input and Multi-Step Output**\n",
    "       7. Spike-sorted/single-unit -> Spike-sorted/single-unit\n",
    "      \n",
    "#### Also \n",
    "I could combine the two and feed the voltage prediction back into another CNN that spits out a binary class (instead of feeding it into a seizure classification algorithm).\n",
    "\n",
    "This could be applied to options 4-7 to produce models 8-11 **for a total of 11 models to evaluate**.\n",
    "\n",
    "And this is only the temporal CNN option. I could also try a [Multilayer Perceptron Model](https://machinelearningmastery.com/how-to-develop-multilayer-perceptron-models-for-time-series-forecasting/)\n",
    ", [LSTM Model](https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/), Seq2Seq, TCN, GRU\n",
    "and each of these could be trained the different ways as above as well. 3 x 11 = 33.\n",
    "\n",
    "And there are more models and more combos. ahhh. hopefully a pattern in the options will quickly make clear what is best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb80f3d-6ada-48a3-b759-4f26043f44b2",
   "metadata": {},
   "source": [
    "I asked chatgpt what it thought and it was a helpful summary and agreement with my thoughts:\n",
    "\n",
    "*What I asked it:* if given frequencies, is it easier for the model to learn the relationship between the current and future waves and predict the next parts of the waves which can then be classified as a siezure or not, or would it be easier to only tell it yes or no if the current waves came before a seizure so it couldn't learn a relationship between the current and future waves it could only learn a relationship betwen the current waves and a binary which seems like it has a less of a relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5b8b66-0a6a-4865-a5d1-86d27419f3be",
   "metadata": {},
   "source": [
    "[source](https://www.baeldung.com/cs/convolutional-vs-regular-nn) \n",
    "\n",
    "# Artificial neurons: \n",
    "base of all neural networks; the units inspired by biological neurons\n",
    "In CNN's they are arranged into 2D or ND filters/kernels. Each filter extracts different features about the data.\n",
    "\n",
    "*The output of a neuron that goes to 1+ other neurons or is used for prediction* = \n",
    "\n",
    "#### activation_function ( bias + sum ( input x connection_weights ) \n",
    "\n",
    "##### bias =  an input to all the nodes and always has the value 1. It allows to shift the result of the activation function to the left or right. It also helps the model to train when all the input features are 0.\n",
    "\n",
    "*This computation is aka The Forward Pass and is all you run later when just making predictions.*\n",
    "\n",
    "Activation f(x)'s are applied as the final step to get the neruon's ouput. This step increases non-linearity in the data to speed up training:\n",
    "- sigmoid - almost always use it as an output neuron in binary classification\n",
    "-  hyperbolic tangent function - better idea for the hidden layers. The most significant difference is that the codomain of the sigmoid is between 0 and 1, while the codomain of the hyperbolic tangent is between -1 and 1. The hyperbolic tangent empirically produces better results because the mean value of the output from this function is closer to zero, which centers data and allows for more accessible learning in the next layer.\n",
    "\n",
    "One drawback of both these activation functions is that are susceptible to \n",
    "\n",
    "the vanishing gradient problem: they have a tiny gradient value for a bigger input value, implying that the function’s slope is close to zero at these points.\n",
    "\n",
    "It can be solved by normalizing the input data to a suitable range or using different activation functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e540f625-6f30-4bb5-a5cb-1fe9de222411",
   "metadata": {},
   "source": [
    "# Activation Functions:\n",
    "\n",
    "Some common ones:\n",
    "- **rectified linear unit (ReLU)** - superior activation f(x) that doesn't do this.\n",
    "- **leaky ReLU (LReLU)** - may help when the data has a lot of noise or outliers: Leaky ReLU can provide a non-zero output for negative input values, which can help to avoid discarding potentially important information (the dying neurons problem). ReLU is currently advised to try 1st, but can also try LReLU which decreases sparsity potentially preventing underfitting but doesn't consistently outperform ReLU it souonds like\n",
    "- **exponential LU ELU** - also prevents the dying neuron problem but also can blow up activation when input gets >1\n",
    "\n",
    "[Note](https://medium.com/@nerdjock/convolutional-neural-network-lesson-9-activation-functions-in-cnns-57def9c6e759#:~:text=In%20a%20CNN%2C%20activation%20functions,not%20applied%20after%20pooling%20layers.)\n",
    "In a CNN, activation functions are typically applied after each convolutional layer and fully connected layer. However, they are not applied after pooling layers.\n",
    "The purpose of using activation functions after the convolutional and fully connected layers is to introduce non-linearity into the model after performing **linear operations (convolution and matrix multiplication).**\n",
    "In essence, **activation functions serve as the “switch” in artificial neurons that decide whether that neuron should be activated or not based on the weighted sum of the input**. This reflects how neurons in the human brain work: they either fire, or they don’t. This biological analogy helps to conceptualize the role of activation functions in a CNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bca490a-0cc3-4deb-a21e-f806a21f40b9",
   "metadata": {},
   "source": [
    "V informative (how the computation works, stride, padding, examples of how the filter slides and what dimensions to expect based on what input): [Applied Deep Learning - Part 4: Convolutional Neural Networks](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "\n",
    "[source](https://www.baeldung.com/cs/convolutional-vs-regular-nn)\n",
    "\n",
    "There are many types of neural networks, but broadly, we can divide them into three classes:\n",
    "\n",
    "1. **FCNN: Fully connected neural networks** (regular neural networks)\n",
    "   - aka ***ANN: Artificial Neural Network***\n",
    "   - fully connected layers and non-linearities\n",
    "   - also sometimes called a **MLP: Multi-Layer Perceptron** which was the first type of FCNN/ANN (and I think it actually just uses a step-f(x) not a non-linearity f(x)). A **DNN (deep neural network)** is just a MLP/FCNN/ANN with \"more\" layers to solve \"more\" complex problems\n",
    "3. **CNN: Convolutional neural networks**\n",
    "   - not connected to the entire input at once, but just series of sections of it which is combined to form a feature map\n",
    "   - [multiple convolutions](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2) on an input, each using a different filter and resulting in a distinct feature map. We then stack all these feature maps together and that becomes the final output of the convolution layer.\n",
    "   - note that the depth of the convolution filter must match the depth of the image, both being 3\n",
    "5. **RNN: Recurrent neural networks**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957cf835-6a3b-407f-af86-0256a8beb0b0",
   "metadata": {},
   "source": [
    "__1D__ CNN: CNN that __operates over a 1D sequence__\n",
    "- it usually has 1 hidden layer first\n",
    "- then perhaps a 2nd convolutional layer\n",
    "- if it's a very long input sequence, maybe a pooling layer after those\n",
    "\n",
    "    - convolutional layer:\n",
    "          - Def a) apply a set of learnable filters (kernels) to the input data. This operation detects features such as edges, textures, shapes, and patterns.\n",
    "          - [Def b)](https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/) A convolution is the simple **application of a filter** to an input that results in an activation. **Repeated application of the same filter to an input results in a map of activations called a feature map**, indicating the locations and strength of a detected feature in an input, such as an image\n",
    "      In the context of a convolutional neural network, **a convolution is a linear operation that involves the multiplication of a set of weights with the input, much like a traditional neural network. Given that the technique was designed for two-dimensional input, the multiplication is performed between an array of input data and a two-dimensional array of weights, called a filter or a kernel**.\n",
    "    - **hidden layer**: any layer that's not the output or input layer of the model\n",
    "    - **pooling layer**: downsample each feature map independently, reducing the height and width, keeping the depth intact. Reduces the computational load and control overfitting. Types:\n",
    "    - -   Max Pooling: Takes the maximum value from a region (e.g., 2x2) in the feature map.\n",
    "\t- - \tAverage Pooling: Computes the average of values in a region."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cf5e65-8911-48c4-8b5e-edf3bc183781",
   "metadata": {},
   "source": [
    "### In CNN architectures, pooling is typically performed with 2x2 windows, stride 2 and no padding. While convolution is done with 3x3 windows, stride 1 and with padding.\n",
    "[source](https://towardsdatascience.com/applied-deep-learning-part-4-convolutional-neural-networks-584bc134c1e2)\n",
    "\n",
    "Parameters:\n",
    "- Filter size: we typically use 3x3 filters, but 5x5 or 7x7 are also used depending on the application. There are also 1x1 filters\n",
    "- Filter count: this is the most variable parameter, it’s a power of two anywhere between 32 and 1024. Using more filters results in a more powerful model, but we risk overfitting due to increased parameter count. Usually we start with a small number of filters at the initial layers, and progressively increase the count as we go deeper into the network.\n",
    "- Stride: we keep it at the default value 1.\n",
    "- Padding: we usually use padding.\n",
    "\n",
    "    After the convolution + pooling layers we add a couple of fully connected layers to wrap up the CNN architecture. The output of both convolution and pooling layers are 3D volumes, but a fully connected layer expects a 1D vector of numbers. So we flatten the output of the final pooling layer to a vector and that becomes the input to the fully connected layer. Flattening is simply arranging the 3D volume of numbers into a 1D vector, nothing fancy happens here.\n",
    "\n",
    "  ### Dropout\n",
    "  By far the most popular regularization technique for deep neural networks. Even the state-of-the-art models which have 95% accuracy get a 2% accuracy boost just by adding dropout, which is a fairly substantial gain at that level.\n",
    "\n",
    "  The hyperparameter p is called the dropout-rate and it’s typically a number around 0.5, corresponding to 50% of the neurons being dropped out.\n",
    "\n",
    "  Almost all state-of-the-art deep networks now incorporate dropout. There is another very popular regularization technique called batch normalization\n",
    "\n",
    "  We are overfitting despite the fact that we are using dropout. The reason is we are training on very few examples, 1000 images per category. Usually we need at least 100K training examples to start thinking about deep learning.  But can try data augmentation\n",
    "\n",
    "  Data augmentation can boost the size of the training set by even 50x. It’s a very powerful technique that is used in every single image-based deep learning model, no exceptions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e42a31-7ea3-4f8a-966f-7fab88eb659a",
   "metadata": {},
   "source": [
    "## _\"Choosing the right number of hidden layers and nodes per layer is more of an art than science, usually decided by trial and error.\"_ [-source](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d70cca-9b20-41c6-8530-22e41d5404ee",
   "metadata": {},
   "source": [
    "## An advantage of stacking two convolutions instead of one is that we use two relu operations, and more non-linearity gives more power to the model.\n",
    "\n",
    "\t•\tThe first 3x3 filter affects a 3x3 region.\n",
    "\t•\tThe second 3x3 filter affects a 3x3 region of the output of the first one, indirectly affecting a 5x5 region of the input.\n",
    "\t•\tThe third 3x3 filter, in turn, affects a 3x3 region of the output of the second one, which corresponds to an effective 7x7 region in the original input.\n",
    "\n",
    "Why This Matters\n",
    "\n",
    "\t•\tParameter Efficiency: Using multiple 3x3 filters instead of a single larger filter (like a 5x5 or 7x7) is more parameter-efficient.\n",
    "\t•\tFor example, a single 5x5 filter has 25 parameters (plus a bias term), whereas two 3x3 filters have 18 parameters (9 for each layer, plus biases).\n",
    "\t•\tThis makes the network lighter, reduces overfitting, and increases its capacity to learn more complex patterns.\n",
    "\t•\tNon-linearity: Using multiple layers of 3x3 convolutions introduces additional non-linearity between the layers (due to activation functions like ReLU). This can improve the network’s ability to learn complex functions, making it more expressive and effective at capturing patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbe018b-2ef8-44ad-8d1e-0dd6194c1d58",
   "metadata": {},
   "source": [
    "# 2 Types of Linear Regression:\n",
    "1) (LR) Logistic Regression - binary classification\n",
    "2) (SR) Softmax Regression - multi-class classification\n",
    "\n",
    "LR uses 1 node in the initial dense/input layer, where SR has 3 nodes (if you have 3 classes). Since we have 3 classes it makes sense for SR to be using 3 nodes. Then the question is, why does LR uses only 1 node, it has 2 classes so it appears like we should have used 2 nodes instead. The answer is, because we can achieve the same result with using only 1 node. As we saw above, **LR models the probability of an example belonging to class one: P(class=1). And we can calculate class 0 probability by: 1−P(class=1). But when we have more than 2 classes, we need individual nodes for each class.** Because knowing the probability of one class doesn’t let us infer the probability of the other classes.\n",
    "\n",
    "LR used sigmoid activation function, SR uses softmax. __Softmax scales the values of the output nodes such that they represent probabilities and sum up to 1.__ So in our case P(class=0)+P(class=1)+P(class=2)=1. It doesn’t do it in a naive way by dividing individual probabilities by the sum though, it uses the exponential function. So higher values get emphasized more and lower values get squashed more. We will talk in detail what softmax does in another tutorial. For now you can simply think of it as a normalization function which lets us interpret the output values as probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8276fe71-8667-455b-a962-365a31cc9798",
   "metadata": {},
   "source": [
    "# Binary vs Multi-class Classification \n",
    "1. Binary class models output nodes always have LR/sigmoid activation f(x)\n",
    "2. Multi-class models output nodes \"always\" use SR/(a complex exponential) activation f(x)\n",
    "\n",
    "# Linear vs Non-linear models\n",
    "1. Linear models typically have no hidden layers. They make predictions based on a linear combination of input features. The output is a direct weighted sum of inputs, without any non-linear transformations. (For a classification proble this goes into the sigmoid f(x)). Examples:\n",
    "   - linear regression and classifiers\n",
    "   - SVM\n",
    "3. Non-linear models can learn non-linearity because of the non-linear activation functions applied to the output of every node. Examples\n",
    "- decision trees\n",
    "- SVM with non-linear kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc0d6f6-96d8-428b-a6b8-e5c4995b2e7e",
   "metadata": {},
   "source": [
    "## Data Normalization\n",
    "\n",
    "By inspecting the histograms of features we can see which ones need normalization. What’s the motivation behind this? What does normalization mean and why is it needed? Most ML algorithms perform better if the real valued features are scaled to be in a predefined range, for example [0, 1]. This is particularly important for deep neural networks. If the input features consist of large values, deep nets really struggle to learn. The reason is that as the data flows through the layers, with all the multiplications and additions, it gets large very quickly and this negatively affects the optimization process by saturating non-linearities. -[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)\n",
    "\n",
    "To Chatgpt: \"how to know if you need to do feature normalization and what does that look like?\"\n",
    "\n",
    "Feature normalization is a preprocessing step that transforms the range of feature values to improve model performance and convergence, especially in models sensitive to feature scale. Here’s how to know if it’s necessary and what it looks like:\n",
    "\n",
    "When to Do Feature Normalization\n",
    "\n",
    "1. If Using Distance-Based Models:\n",
    "   - Examples: k-Nearest Neighbors (k-NN), Support Vector Machines (SVMs), and clustering algorithms like K-means.\n",
    "     - Reason: Distance-based models are highly affected by the scale of features. Features with large ranges will dominate the distance calculation, skewing the results.\n",
    "2. Gradient Descent-Based Models:\n",
    "    - Examples: Neural networks and linear regression optimized with gradient descent.\n",
    "    \t- Reason: Scaling helps gradient descent converge faster and more smoothly, as unscaled features can lead to unstable updates and long convergence times.\n",
    "3. When Features Have Different Ranges:\n",
    "\t- If one feature ranges from 0 to 1 and another from 0 to 10,000, the model may give more weight to the larger-scaled feature. Normalization ensures all features contribute more evenly.\n",
    "4. Regularization Impact:\n",
    "\t- Models with regularization terms (e.g., Lasso, Ridge regression) also benefit, as unnormalized features can lead to skewed penalty terms, affecting model performance.\n",
    "\n",
    "### Types of Feature Normalization\n",
    "\n",
    "1. _Standardization_ (Z-score **Normalization**):\n",
    "\t- Formula: **(value - mean)/std_dev**\n",
    "    \t- Use: Centers the data to have a mean of 0 and standard deviation of 1. This is useful when the data follows a normal distribution or when algorithms assume a Gaussian distribution (e.g., linear regression, logistic regression).\n",
    "2. _Min-Max **Scaling**_:\n",
    "\t- Formula: **(x - xmin)/(xmax - xmin)**\n",
    "\t- Range: Often scales features to a range of [0, 1] or [-1, 1].\n",
    "    \t- Use: **Commonly applied to neural networks** or when features have no strict assumptions about the distribution.\n",
    "3. _Max Abs **Scaling**_:\n",
    "\t- Formula: **x/|xmax|** (where || denotes absolute value)\n",
    "    \t- Use: Scales data to the range [-1, 1] without shifting the center to zero, which is useful when dealing with sparse data like term frequencies in text analysis.\n",
    "4. _Robust **Scaling**_:\n",
    "\t- Formula: **(x - xmedian)/(Interquartile Range)**\n",
    "    \t- Use: Less sensitive to outliers, making it suitable when the data contains extreme values or a skewed distribution.\n",
    "\n",
    "#### Min_Max [-1,1] vs Z-score normalization: \n",
    "\n",
    "Z-score normalization doesn’t necessarily map values into a [0, 1] or [-1, 1] range but rather makes the data have a mean of 0 and a standard deviation of 1. It’s often used when the dataset has outliers or does not need to be strictly bounded.\n",
    "\n",
    "\n",
    "Scaling vs. Normalization: When scaling data to [0, 1] or [-1, 1], you’re transforming the range based on the data’s min and max values rather than its distribution’s mean and variance. This approach maintains the data’s structure while constraining its range, which can be particularly useful for algorithms sensitive to the scale of the data (e.g., SVMs, neural networks).\n",
    "\n",
    "\n",
    "#### Practical Example: Applying Normalization\n",
    "\n",
    "Suppose you have a dataset with features height (in cm) and weight (in kg), which vary significantly in range. For a neural network, you could use Min-Max Scaling:\n",
    "\n",
    "\t1.\tCalculate the min and max for each feature.\n",
    "\t2.\tApply the Min-Max formula to each value, bringing all data to the [0, 1] range.\n",
    "\n",
    "Alternatively, for SVM or linear regression, you could use Standardization to ensure both features have a mean of 0 and standard deviation of 1, making optimization more efficient and less skewed by feature scale.\n",
    "\n",
    "#### When You Might Skip Normalization\n",
    "\n",
    "\t•\tTree-based models: Algorithms like Decision Trees, Random Forests, and Gradient Boosting generally don’t require feature normalization because they are not affected by feature scale.\n",
    "\t•\tAlready normalized data: If features are already in the same scale or distribution (e.g., pixel intensities in image data for many models), you might not need additional normalization.\n",
    "\n",
    "In summary, normalization helps when feature scales vary widely, or models depend on feature scale for performance and convergence. Select a normalization method based on the model type and data distribution.\n",
    "\n",
    "\n",
    "#### Also note:\n",
    "__If the feature values are small, we could get away without any normalization, but it’s a good habit to do so, and there’s no harm doing it anyway.__ Pairplot is a cool visualization technique if we have a small number of features. We can see the pairwise distribution of features colored by the class (the column “label” in the dataset). We use the seaborn package, pretty simple for an informative plot.[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cfb619-68bf-48fd-8189-630e5f7549f1",
   "metadata": {},
   "source": [
    "# Converting categorical data via _One-Hot Encoding_\n",
    "\n",
    " We need to convert categorical data to one-hot representation. For example the salary column contains 3 unique string values: low, medium and high. After one-hot conversion we will have 3 new binary columns: salary_low, salary_medium and salary_high. For a given example, only one of them will have the value 1, the others will be 0. We will then drop the original salary column because we don’t need it anymore.\n",
    "The one-hot conversion is performed by the get_dummies of pandas. We could have also used the OneHotEncoder in scikit-learn, they both get the job done. -[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f044d42d-12ce-4176-ab6b-2b05da7b17be",
   "metadata": {},
   "source": [
    "# Bucketization\n",
    "\n",
    " For example the feature which contains the year the house was built (yr_built), ranges from 1900 to 2015. We can certainly categorize it with every year belonging to a distinct category, but then it would be pretty sparse. We would get more signal if we bucketized this feature without losing much information. For example if we use 10 year buckets, years between [1950, 1959] would be collapsed together. It would probably be sufficient to know that this house was built in 1950s instead of 1958 exactly.\n",
    "Other features that would benefit from bucketizing are latitude and longitude of the house. The exact coordinate doesn’t matter that much, we can round the coordinate to the nearest kilometer. This way the feature values will be more dense and informative. There’s no hard and set rule to which ranges to use in bucketization, they’re mostly decided by trial and error.\n",
    "One final transformation we need to do is for the price of the house, the value we’re trying to predict. Currently its value ranges from $75K to $7.7M. A model trying to predict in such a large scale and variance would be very unstable. So we normalize that as well. Feel free to check the code for the details. -[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cca5ff-f5c4-4fe5-9495-a9bc06721e30",
   "metadata": {},
   "source": [
    "# Other Ways to Improve your Model\n",
    "\n",
    "We can further improve the performance of the ANN by doing the following:\n",
    "- Train the model for longer (increase the number of epochs).\n",
    "- Hyperparamter tuning: change the learning rate, use a different optimizer than Adam (RMSprop for example), use another activation function than tanh (can be relu).\n",
    "- Increase the number of nodes per layer: Instead of 64–16–1 we can do 128–64–1.\n",
    "- Increase the number of layers: We can do 128–64–32–16–1.\n",
    "\n",
    "Why don't we always go crazy with number of layers and nodes per layer? Becuase it can also result in overfitting. The simplest model that gets the job done is sufficient.\n",
    "\n",
    "[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8513df-0466-4d13-bc7d-0e5a4f129425",
   "metadata": {},
   "source": [
    "# Multivariate CNN Models\n",
    "\n",
    "Multivariate time series data means data where there is more than one observation for each time step.\n",
    "\n",
    "There are two main models that we may require with multivariate time series data; they are:\n",
    "1. Multiple Input Series.\n",
    "2. Multiple Parallel Series."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab746b7-8126-4506-998e-8e80ade2c862",
   "metadata": {},
   "source": [
    "# How to project multi-dimensional data into 2D space to visualize\n",
    "\n",
    "For visualization purposes let’s project it to 2D. Remember that **having k nodes in a layer means that this layer transforms its input such that the output is a k-dimensional vector.** The ANN we trained above had two hidden layers with 64 and 16 nodes. Then **we need a new layer with 2 nodes in order to project our data to a 2D space**. So we add this new layer just before the output node. The rest is completely untouched.\n",
    "\n",
    "A more principled visualization approach would be using t-Distributed Stochastic Neighbor Embedding (t-SNE), which is a dimensionality reduction technique for visualizing high-dimensional data. Details available [here](https://lvdmaaten.github.io/tsne/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b530c-983e-4687-9aff-66f25f1309ac",
   "metadata": {},
   "source": [
    "# Regression: \n",
    "predicting a real-valued output instead of discreet class memberships. \n",
    "\n",
    "the equation for logistic regression is y=f(xW) where f is the sigmoid function. Linear regression is simply y=xW, that’s it no activation function. I’ve again omitted the bias terms for simplicity. With the biases they become y=f(xW+b) and y=xW+b respectively. \n",
    "\n",
    "There are 3 main differences:\n",
    "1. LR uses a sigmoid activation function, where LinR has no activation.\n",
    "2. LR uses binary_crossentropy loss function, where LinR uses mean_squared_error.\n",
    "3. LR also reports the accuracy, but accuracy is not an applicable metric to a regression problem, since the output is a floating point number instead of a class membership.\n",
    "\n",
    "\n",
    "[source](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e11097-6e2c-4140-b606-fe78f34832b5",
   "metadata": {},
   "source": [
    "#  Cross Validation\n",
    "With a small sample size like our current situation, it’s especially important to perform cross validation to get a better estimate on accuracy. With k-fold cross validation we split the dataset into k disjoint parts, use k-1 parts for training and the other 1 part for testing. This way every example appears in both training and test sets. We then average out the model’s performance in all k runs and get a better low-variance estimation of the model accuracy.\n",
    "\n",
    "Usually while training deep learning models we don’t perform k-fold cross validation. Because the training takes a long time, and training the model k times from scratch is not feasible. But since our dataset is small it’s a good candidate to try on.\n",
    "\n",
    "Go to section in the [article](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585) for a v helpful visualization if you don't get what they're saying bc I didn't before I saw the image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0c29f7-80cc-4077-8278-2527eef54c59",
   "metadata": {},
   "source": [
    "# Final Eval\n",
    "\n",
    "Compare the prediction ability of the most naive model possible (static prediction of the average price, static prediction of the most common class,etc) to a linear model, an ANN, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14fa14-487b-4204-ab95-8c44f59b2e94",
   "metadata": {},
   "source": [
    "# Autoencoders \n",
    "[autoencoders article source](https://towardsdatascience.com/applied-deep-learning-part-3-autoencoders-1c083af4d798)\n",
    "\n",
    "Autoencoders are a specific type of feedforward neural networks where the input is the same as the output.\n",
    "\n",
    "They have 3 common uses:\n",
    "1. Data denoising\n",
    "2. Dimensionality reduction: visualizing high-dimensional data is challenging. t-SNE is the most commonly used method but struggles with large number of dimensions (typically above 32). So autoencoders are used as a preprocessing step to reduce the dimensionality, and this compressed representation is used by t-SNE to visualize the data in 2D space. For great articles on t-SNE refer [here](https://distill.pub/2016/misread-tsne/) and [here](http://colah.github.io/posts/2014-10-Visualizing-MNIST/).\n",
    "3. Variational Autoencoders (VAE): this is a more modern and complex use-case of autoencoders and we will cover them in another article. But as a quick summary, VAE learns the parameters of the probability distribution modeling the input data, instead of learning an arbitrary function in the case of vanilla autoencoders. By sampling points from this distribution we can also use the VAE as a generative model. [Here](https://kvfrans.com/variational-autoencoders-explained/) is a good reference.\n",
    "\n",
    "\n",
    "They compress the input into a lower-dimensional code and then reconstruct the output from this representation. _The code is a compact “summary” or “compression” of the input,_ **also called the latent-space representation**.\n",
    "\n",
    "An autoencoder consists of 3 components: \n",
    "1. encoder\n",
    "2. code\n",
    "3. decoder\n",
    "\n",
    "To build you need 3 things: \n",
    "1. an encoding method\n",
    "2. a decoding method\n",
    "3. a loss function to compare the output with the target. We either use mean squared error (mse) or binary crossentropy. If the input values are in the range [0, 1] then we typically use crossentropy, otherwise we use the mean squared error. \n",
    "\n",
    "\n",
    "Autoencoders are mainly a dimensionality reduction (or compression) algorithm but are \n",
    "1) Data-specific\n",
    "2) Lossy\n",
    "3) Unsupervised (self-supervised really)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe102def-e415-451a-8bd7-4d203f8a0da4",
   "metadata": {},
   "source": [
    "## Denoising Autoencoders\n",
    "Keeping the code layer small forced our autoencoder to learn an intelligent representation of the data. There is another way to **force the autoencoder to learn useful features, which is adding random noise to its inputs and making it recover the original noise-free data.** This way the autoencoder can’t simply copy the input to its output because the input also contains random noise. We are asking it to subtract the noise and produce the underlying meaningful data. This is called a denoising autoencoder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bf157b-31b8-4ed3-80b2-3693b8c368b0",
   "metadata": {},
   "source": [
    "## Sparse Autoencoders\n",
    "We introduced two ways to force the autoencoder to learn useful features: keeping the code size small and denoising autoencoders. The third method is using regularization. **We can regularize the autoencoder by using a sparsity constraint such that only a fraction of the nodes would have nonzero values, called active nodes.**\n",
    "\n",
    "\n",
    "_In particular, **we add a penalty term to the loss function** such that only a fraction of the nodes become active._ This forces the autoencoder to represent each input as a combination of small number of nodes, and demands it to discover interesting structure in the data. This method works even if the code size is large, since only a small subset of the nodes will be active at any time.\n",
    "\n",
    "By adding an activity_regularizer term, say 0.01, the final loss of the sparse model is 0.01 higher than the standard one, due to the added regularization term."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b9af3e-d6f6-42db-8242-70296e97ca95",
   "metadata": {},
   "source": [
    "# RNN's (Recurrent Neural Networks)\n",
    "\n",
    "[RNN info source](https://towardsdatascience.com/recurrent-neural-networks-rnns-3f06d7653a85)\n",
    "\n",
    "Processes sequential data.\n",
    "\n",
    "Steps:\n",
    "- Input: x(t)​ is taken as the input to the network at time step t. For example, x1,could be a one-hot vector corresponding to a word of a sentence.\n",
    "- Hidden state: h(t)​ represents a hidden state at time t and acts as “memory” of the network. h(t)​ is calculated based on the current input and the previous time step’s hidden state: h(t)​ = f(U x(t)​ + W h(t−1)​). The function f is taken to be a non-linear transformation such as tanh, ReLU.\n",
    "- Weights: The RNN has input to hidden connections parameterized by a weight matrix U, hidden-to-hidden recurrent connections parameterized by a weight matrix W, and hidden-to-output connections parameterized by a weight matrix V and all these weights (U,V,W) are shared across time.\n",
    "- Output: o(t)​ illustrates the output of the network. In the figure I just put an arrow after o(t) which is also often subjected to non-linearity, especially when the network contains further layers downstream."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f559fa-f40f-4ac8-8263-044615d03fc0",
   "metadata": {},
   "source": [
    "### Softmax example from chatgpt\n",
    "\n",
    "Suppose we have three classes with logits (raw output scores) [2.0, 1.0, 0.1]. Applying softmax:\n",
    "\n",
    "\t1.\tExponentiate each score: e^{2.0} \\approx 7.39, e^{1.0} \\approx 2.72, e^{0.1} which is aprox= 1.11.\n",
    "\t2.\tSum of exponentiated scores: 7.39 + 2.72 + 1.11 = 11.22.\n",
    "\t3.\tCompute probabilities:\n",
    "\t•\tClass 1: \\frac{7.39}{11.22} \\approx 0.66\n",
    "\t•\tClass 2: \\frac{2.72}{11.22} \\approx 0.24\n",
    "\t•\tClass 3: \\frac{1.11}{11.22} \\approx 0.10\n",
    "\n",
    "Thus, the softmax output is approximately [0.66, 0.24, 0.10], suggesting the model is most confident in Class 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48ace73-149a-4a19-b10a-417297e94698",
   "metadata": {},
   "source": [
    "### Vanishing Gradient Problem:\n",
    "\n",
    "[another RNN source](https://medium.com/@sachinsoni600517/recurrent-neural-networks-rnn-from-basic-to-advanced-1da22aafa009)\n",
    "This is a major hurdle for RNNs, especially vanilla RNNs. It occurs when gradients (signals used to update weights during training) become very small or vanish as they propagate backward through the network during BPTT. This makes it difficult for the network to learn long-term dependencies in sequences, as information from earlier time steps can fade away.\n",
    "\n",
    "Gradients are multiplied by weights at each step. If these weights are all less than 1 in absolute value, this product becomes progressively smaller as we go back in time. This shrinks the error gradients for earlier time steps, making it difficult for the network to learn long-term dependencies present in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a390c92-dc36-432d-9fad-978f772d3c60",
   "metadata": {},
   "source": [
    "# LSTM CNN's"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd52091d",
   "metadata": {},
   "source": [
    "# TCN - Temporal Neural Nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1cb43",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
